{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import pandas\n",
    "import sklearn.ensemble\n",
    "import sklearn.linear_model\n",
    "import sklearn.model_selection\n",
    "\n",
    "#import xgboost - Not working on Rob's laptop\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# General setup.\n",
    "\n",
    "# Return mean absolute error scale.\n",
    "def mean_absolute_error_salary_scale(y_test, y_predicted):\n",
    "    return sklearn.metrics.mean_absolute_error(\n",
    "        numpy.exp(y_test), numpy.exp(y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define training and test data.\n",
    "\n",
    "data = pandas.read_csv('data/train_large.csv')\n",
    "\n",
    "X_train_index, X_test_index, Y_train, Y_test = sklearn.model_selection.train_test_split(\n",
    "    data.index, data['LogSalaryNormalized'], test_size=.3, random_state=42)\n",
    "\n",
    "# Define LDA data.\n",
    "lda_data = pandas.read_csv('data/train_large_lda_50.csv')\n",
    "\n",
    "X_train_lda = lda_data.iloc[X_train_index]\n",
    "X_test_lda = lda_data.iloc[X_test_index]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Regressor Mean Absolute Error:  8380.5428\n"
     ]
    }
   ],
   "source": [
    "# Model: Random Forests\n",
    "# Data Representation: LDA\n",
    "# Library used: sklearn\n",
    "\n",
    "# NOTE: Have since retrained LDA using sklearn and online mini-batch learning.\n",
    "\n",
    "rf = sklearn.ensemble.RandomForestRegressor(\n",
    "    n_estimators=1000, n_jobs=-1, warm_start=True)\n",
    "\n",
    "rf.fit(X_train_lda, Y_train)\n",
    "rf_predictions = rf.predict(X_test_lda)\n",
    "\n",
    "# Scale to get Mean Absolute Error\n",
    "rf_mae = mean_absolute_error_salary_scale(Y_test, rf_predictions)\n",
    "\n",
    "print('Random Forest Regressor Mean Absolute Error: {:10.4f}'.format(rf_mae))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interesting note: Using LDA, n_estimators = 10 performs better than 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model: XGBoost\n",
    "# Data Representation: LDA\n",
    "# Library used: xgboost\n",
    "\n",
    "# NOTE: xgboost will not import on Rob's laptop.\n",
    "\n",
    "xgb = xgboost.XGBRegressor(\n",
    "        learning_rate=.10, max_depth=10, n_estimators=1000, silent=False,\n",
    "        subsample=.8, colsample_bytree=.8, nthread=4, min_child_weight=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDRegressor Mean Absolute Error:  8929.7799\n"
     ]
    }
   ],
   "source": [
    "# Model: SGD\n",
    "# Data Representation: LDA\n",
    "# Library used: sklearn\n",
    "\n",
    "# Normalization not required as LDA output is normalised between 0 and\n",
    "# 1 anyway.\n",
    "\n",
    "# We want a stochastic gradient descent with l1 norm.\n",
    "sgd = sklearn.linear_model.SGDRegressor(\n",
    "    alpha=.0001, penalty='l1', n_iter=10000)\n",
    "sgd.fit(X_train_lda, Y_train)\n",
    "sgd_predictions = sgd.predict(X_test_lda)\n",
    "sgd_mae = mean_absolute_error_salary_scale(Y_test, sgd_predictions)\n",
    "print('SGDRegressor Mean Absolute Error: {:10.4f}'.format(sgd_mae))\n",
    "#TODO (any): wonder why this is so inaccurate/wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows trained on:  5000.0000\n",
      "Number of trees in forest:  1000.0000\n",
      "Random Forest Regressor Mean Absolute Error:  8705.0733\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Model: Random Forests using batch learning\n",
    "# Data Representation: LDA\n",
    "# Library used: sklearn\n",
    "\n",
    "# NOTE: Have since retrained LDA using sklearn and online mini-batch learning.\n",
    "\n",
    "def batch_processed_rf(X_train, Y_train, batch_size=1000, n_estimators_per_batch=100):\n",
    "    \"\"\"Function to process random forest in batches using warm start.\"\"\"\n",
    "    # Define model.\n",
    "    n_estimators = n_estimators_per_batch\n",
    "    total_processed_rows = 0\n",
    "    \n",
    "    rf = sklearn.ensemble.RandomForestRegressor(\n",
    "    n_estimators=n_estimators_per_batch, n_jobs=-1, warm_start=True)\n",
    "    \n",
    "    # Reindex training and test data.\n",
    "    # Should randomize here, but we have already taken this step, so no need in our case.\n",
    "    X_train.reset_index(drop=True)\n",
    "    Y_train.reset_index(drop=True)\n",
    "    \n",
    "    def run_batch(X_train_batch, Y_train_batch, n_estimators, total_processed_rows):     \n",
    "        \"\"\"Function to run next batch of random forest.\"\"\"\n",
    "        processed_rows = total_processed_rows + len(X_train_batch)\n",
    "        rf.set_params(n_estimators=n_estimators)\n",
    "        rf.fit(X_train_batch, Y_train_batch)\n",
    "        rf_predictions = rf.predict(X_test_lda)\n",
    "        rf_mae = mean_absolute_error_salary_scale(Y_test, rf_predictions)\n",
    "        print('Number of rows trained on: {:10.4f}'.format(processed_rows))\n",
    "        print('Number of trees in forest: {:10.4f}'.format(n_estimators))\n",
    "        print('Random Forest Regressor Mean Absolute Error: {:10.4f}'.format(rf_mae))\n",
    "        print('')\n",
    "    \n",
    "    while total_processed_rows < len(X_train) - batch_size:     \n",
    "        # Scale to get Mean Absolute Error\n",
    "        batch_index = range(total_processed_rows, total_processed_rows + batch_size)\n",
    "        X_train_batch = X_train.iloc[batch_index]\n",
    "        Y_train_batch = Y_train.iloc[batch_index]\n",
    "        run_batch(X_train_batch, Y_train_batch, n_estimators, total_processed_rows)\n",
    "        total_processed_rows += batch_size\n",
    "        n_estimators += n_estimators_per_batch\n",
    "        \n",
    "    remaining_rows = len(X_train) - total_processed_rows\n",
    "    if remaining_rows == 0:\n",
    "        return\n",
    "    batch_index = range(total_processed_rows, len(X_train))\n",
    "    X_train_batch = X_train.iloc[batch_index]\n",
    "    Y_train_batch = Y_train.iloc[batch_index]\n",
    "    run_batch(X_train_batch, Y_train_batch, n_estimators, total_processed_rows)\n",
    "\n",
    "batch_processed_rf(X_train_lda, Y_train, batch_size=5000, n_estimators_per_batch=1000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: With the same data, training in batch results in a higher error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
