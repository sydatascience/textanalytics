{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "import numpy\n",
    "import pandas\n",
    "import sklearn.ensemble\n",
    "import sklearn.feature_extraction\n",
    "import sklearn.linear_model\n",
    "import sklearn.model_selection\n",
    "import tf_utils\n",
    "import xgboost\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# General setup.\n",
    "\n",
    "# Return mean absolute error scale.\n",
    "def mean_absolute_error_salary_scale(y_test, y_predicted):\n",
    "    return sklearn.metrics.mean_absolute_error(\n",
    "        numpy.exp(y_test), numpy.exp(y_predicted))\n",
    "\n",
    "def mae_without_log_scaling(y_test, y_predicted):\n",
    "    return sklearn.metrics.mean_absolute_error(\n",
    "        y_test, y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(137179, 10996)\n",
      "(58792, 10996)\n"
     ]
    }
   ],
   "source": [
    "# Define training and test data.\n",
    "\n",
    "data = pandas.read_csv('data/train.csv')\n",
    "\n",
    "X_train_index, X_test_index, Y_train, Y_test = (\n",
    "    sklearn.model_selection.train_test_split(\n",
    "        data.index, data['SalaryNormalized'],\n",
    "        test_size=.3, random_state=42))\n",
    "\n",
    "# Define BOW data.\n",
    "TRAINING_COLUMN = 'CompleteJobListing'\n",
    "\n",
    "X_train_bow = data.iloc[X_train_index]\n",
    "X_test_bow = data.iloc[X_test_index]\n",
    "\n",
    "count_vect = sklearn.feature_extraction.text.CountVectorizer(\n",
    "    stop_words='english', min_df=50, decode_error='ignore')\n",
    "\n",
    "# Transform BOW test set.\n",
    "X_train_counts = count_vect.fit_transform(\n",
    "    X_train_bow[TRAINING_COLUMN])\n",
    "X_test_counts = count_vect.transform(\n",
    "    X_test_bow[TRAINING_COLUMN])\n",
    "\n",
    "# Normalize BOW\n",
    "normalizer = sklearn.preprocessing.Normalizer(norm='l1')\n",
    "X_train_norm = normalizer.fit_transform(X_train_counts.astype('float64'))\n",
    "X_test_norm = normalizer.transform(X_test_counts.astype('float64'))\n",
    "print(X_train_norm.shape)\n",
    "\n",
    "print(X_test_counts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(156776, 300)\n",
      "(39194, 300)\n"
     ]
    }
   ],
   "source": [
    "# Define training and test data using word2vec\n",
    "\n",
    "word_2_vec_train_data = pandas.read_csv('word2vecdf.csv')\n",
    "word_2_vec_target_data = pandas.read_csv('word2vectargets.csv')\n",
    "\n",
    "X_train_w2v, X_test_w2v, Y_train_w2v, Y_test_w2v = (\n",
    "    sklearn.model_selection.train_test_split(\n",
    "        word_2_vec_train_data, word_2_vec_target_data,\n",
    "        test_size=0.2, random_state=42))\n",
    "\n",
    "print(X_train_w2v.shape)\n",
    "print(X_test_w2v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195971\n",
      "195971\n"
     ]
    }
   ],
   "source": [
    "# Define LDA data.\n",
    "lda_raw = pandas.read_csv('data/train.csv')\n",
    "\n",
    "lda_data = pandas.read_csv('data/train_lda_30.csv')\n",
    "print(len(lda_raw))\n",
    "print(len(lda_data))\n",
    "\n",
    "X_train_lda_index, X_test_lda_index, Y_train_lda, Y_test_lda = (\n",
    "    sklearn.model_selection.train_test_split(\n",
    "        lda_raw.index, lda_raw['LogSalaryNormalized'],\n",
    "        test_size=.3, random_state=42))\n",
    "\n",
    "X_train_lda = lda_data.iloc[X_train_lda_index]\n",
    "X_test_lda = lda_data.iloc[X_test_lda_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13398.6388346\n"
     ]
    }
   ],
   "source": [
    "# Just guess the mean (baseline model)\n",
    "\n",
    "guess_the_mean = tf_utils.MeanEstimator()\n",
    "\n",
    "guess_the_mean.fit(X_train_bow, Y_train)\n",
    "mean_predictions = guess_the_mean.predict(X_test_bow)\n",
    "\n",
    "# Scale to get Mean Absolute Error\n",
    "rf_mae = mae_without_log_scaling(Y_test, mean_predictions)\n",
    "print(rf_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Regressor Mean Absolute Error:  8380.5428\n"
     ]
    }
   ],
   "source": [
    "# Model: Random Forests\n",
    "# Data Representation: LDA\n",
    "# Library used: sklearn\n",
    "\n",
    "# Tried increasing min_samples_leaf and min_samples_split, both did worse\n",
    "\n",
    "rf = sklearn.ensemble.RandomForestRegressor(\n",
    "    n_estimators=1000, n_jobs=-1, warm_start=True)\n",
    "\n",
    "rf.fit(X_train_lda, Y_train_lda)\n",
    "rf_predictions = rf.predict(X_test_lda)\n",
    "\n",
    "# Scale to get Mean Absolute Error\n",
    "rf_mae = mean_absolute_error_salary_scale(Y_test_lda, rf_predictions)\n",
    "\n",
    "print('Random Forest Regressor Mean Absolute Error: {:10.4f}'.format(rf_mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [51416, 6000]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-05cde292fa54>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mrun_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_estimators\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_processed_rows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0mbatch_processed_rf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_lda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_estimators_per_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-20-05cde292fa54>\u001b[0m in \u001b[0;36mbatch_processed_rf\u001b[0;34m(X_train, Y_train, batch_size, n_estimators_per_batch)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mX_train_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mY_train_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mrun_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_estimators\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_processed_rows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mtotal_processed_rows\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mn_estimators\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mn_estimators_per_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-05cde292fa54>\u001b[0m in \u001b[0;36mrun_batch\u001b[0;34m(X_train_batch, Y_train_batch, n_estimators, total_processed_rows)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mrf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mrf_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_lda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mrf_mae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean_absolute_error_salary_scale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrf_predictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Number of rows trained on: {:10.4f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_rows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Number of trees in forest: {:10.4f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-4327542a7438>\u001b[0m in \u001b[0;36mmean_absolute_error_salary_scale\u001b[0;34m(y_test, y_predicted)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmean_absolute_error_salary_scale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_predicted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     return sklearn.metrics.mean_absolute_error(\n\u001b[0;32m----> 6\u001b[0;31m         numpy.exp(y_test), numpy.exp(y_predicted))\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/sklearn/metrics/regression.py\u001b[0m in \u001b[0;36mmean_absolute_error\u001b[0;34m(y_true, y_pred, sample_weight, multioutput)\u001b[0m\n\u001b[1;32m    161\u001b[0m     \"\"\"\n\u001b[1;32m    162\u001b[0m     y_type, y_true, y_pred, multioutput = _check_reg_targets(\n\u001b[0;32m--> 163\u001b[0;31m         y_true, y_pred, multioutput)\n\u001b[0m\u001b[1;32m    164\u001b[0m     output_errors = np.average(np.abs(y_pred - y_true),\n\u001b[1;32m    165\u001b[0m                                weights=sample_weight, axis=0)\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/sklearn/metrics/regression.py\u001b[0m in \u001b[0;36m_check_reg_targets\u001b[0;34m(y_true, y_pred, multioutput)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \"\"\"\n\u001b[0;32m---> 74\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m     \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 181\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [51416, 6000]"
     ]
    }
   ],
   "source": [
    "# Model: Random Forests using batch learning\n",
    "# Data Representation: LDA\n",
    "# Library used: sklearn\n",
    "\n",
    "def batch_processed_rf(X_train, Y_train, batch_size=1000, n_estimators_per_batch=100):\n",
    "    \"\"\"Function to process random forest in batches using warm start.\"\"\"\n",
    "    \n",
    "    def run_batch(X_train_batch, Y_train_batch, n_estimators, total_processed_rows):     \n",
    "        \"\"\"Function to run next batch of random forest.\"\"\"\n",
    "        processed_rows = total_processed_rows + len(X_train_batch)\n",
    "        rf.set_params(n_estimators=n_estimators)\n",
    "        rf.fit(X_train_batch, Y_train_batch)\n",
    "        rf_predictions = rf.predict(X_test_lda)\n",
    "        rf_mae = mean_absolute_error_salary_scale(Y_test, rf_predictions)\n",
    "        print('Number of rows trained on: {:10.4f}'.format(processed_rows))\n",
    "        print('Number of trees in forest: {:10.4f}'.format(n_estimators))\n",
    "        print('Random Forest Regressor Mean Absolute Error: {:10.4f}'.format(rf_mae))\n",
    "        print('')\n",
    "    \n",
    "    n_estimators = n_estimators_per_batch\n",
    "    total_processed_rows = 0\n",
    "    \n",
    "    # Define model.\n",
    "    rf = sklearn.ensemble.RandomForestRegressor(\n",
    "    n_estimators=n_estimators_per_batch, n_jobs=-1, warm_start=True)\n",
    "    \n",
    "    # Reindex training and test data.\n",
    "    # Should randomize here, but we have already taken this step, so no need in our case.\n",
    "    X_train.reset_index(drop=True)\n",
    "    Y_train.reset_index(drop=True)\n",
    "    \n",
    "    while total_processed_rows < len(X_train) - batch_size:     \n",
    "        # Scale to get Mean Absolute Error\n",
    "        batch_index = range(total_processed_rows, total_processed_rows + batch_size)\n",
    "        X_train_batch = X_train.iloc[batch_index]\n",
    "        Y_train_batch = Y_train.iloc[batch_index]\n",
    "        run_batch(X_train_batch, Y_train_batch, n_estimators, total_processed_rows)\n",
    "        total_processed_rows += batch_size\n",
    "        n_estimators += n_estimators_per_batch\n",
    "        \n",
    "    remaining_rows = len(X_train) - total_processed_rows\n",
    "    if remaining_rows == 0:\n",
    "        return\n",
    "    batch_index = range(total_processed_rows, len(X_train))\n",
    "    X_train_batch = X_train.iloc[batch_index]\n",
    "    Y_train_batch = Y_train.iloc[batch_index]\n",
    "    run_batch(X_train_batch, Y_train_batch, n_estimators, total_processed_rows)\n",
    "\n",
    "batch_processed_rf(X_train_lda, Y_train, batch_size=1000, n_estimators_per_batch=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows trained on: 10000.0000\n",
      "Number of trees in forest:   100.0000\n",
      "Random Forest Regressor Mean Absolute Error:  8663.3455\n",
      "\n",
      "Number of rows trained on: 20000.0000\n",
      "Number of trees in forest:   200.0000\n",
      "Random Forest Regressor Mean Absolute Error:  8488.6272\n",
      "\n",
      "Number of rows trained on: 30000.0000\n",
      "Number of trees in forest:   300.0000\n",
      "Random Forest Regressor Mean Absolute Error:  8437.6218\n",
      "\n",
      "Number of rows trained on: 40000.0000\n",
      "Number of trees in forest:   400.0000\n",
      "Random Forest Regressor Mean Absolute Error:  8432.2712\n",
      "\n",
      "Number of rows trained on: 50000.0000\n",
      "Number of trees in forest:   500.0000\n",
      "Random Forest Regressor Mean Absolute Error:  8406.4605\n",
      "\n",
      "Number of rows trained on: 60000.0000\n",
      "Number of trees in forest:   600.0000\n",
      "Random Forest Regressor Mean Absolute Error:  8395.6568\n",
      "\n",
      "Number of rows trained on: 70000.0000\n",
      "Number of trees in forest:   700.0000\n",
      "Random Forest Regressor Mean Absolute Error:  8396.6413\n",
      "\n",
      "Number of rows trained on: 80000.0000\n",
      "Number of trees in forest:   800.0000\n",
      "Random Forest Regressor Mean Absolute Error:  8388.4765\n",
      "\n",
      "Number of rows trained on: 90000.0000\n",
      "Number of trees in forest:   900.0000\n",
      "Random Forest Regressor Mean Absolute Error:  8388.7784\n",
      "\n",
      "Number of rows trained on: 100000.0000\n",
      "Number of trees in forest:  1000.0000\n",
      "Random Forest Regressor Mean Absolute Error:  8384.3816\n",
      "\n",
      "Number of rows trained on: 110000.0000\n",
      "Number of trees in forest:  1100.0000\n",
      "Random Forest Regressor Mean Absolute Error:  8382.4850\n",
      "\n",
      "Number of rows trained on: 120000.0000\n",
      "Number of trees in forest:  1200.0000\n",
      "Random Forest Regressor Mean Absolute Error:  8380.0534\n",
      "\n",
      "Number of rows trained on: 120037.0000\n",
      "Number of trees in forest:  1300.0000\n",
      "Random Forest Regressor Mean Absolute Error:  8551.7053\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Model: Random Forests using batch learning - Tried without batch\n",
    "# learning, just ran forever.\n",
    "# Data Representation: Bag of Words\n",
    "# Library used: sklearn\n",
    "\n",
    "# NOTE: This is just to get this set up and working.\n",
    "# TODO: Move this to be a single function under 'Random Forests' where you simply define the\n",
    "# model as being BOW or LDA.\n",
    "\n",
    "def batch_processed_rf_bow(X_train, Y_train, batch_size=1000, n_estimators_per_batch=100):\n",
    "    \"\"\"Function to process random forest in batches using warm start.\"\"\"\n",
    "    # Define model.\n",
    "    n_estimators = n_estimators_per_batch\n",
    "    total_processed_rows = 0\n",
    "    \n",
    "    rf = sklearn.ensemble.RandomForestRegressor(\n",
    "        n_estimators=n_estimators_per_batch, n_jobs=-1, warm_start=True)\n",
    "   \n",
    "    # Reindex training and test data.\n",
    "    # Should randomize here, but we have already taken this step, so no need in our case.\n",
    "    Y_train.reset_index(drop=True)\n",
    "    \n",
    "    def run_batch(X_train_batch, Y_train_batch, n_estimators, total_processed_rows):     \n",
    "        \"\"\"Function to run next batch of random forest.\"\"\"\n",
    "        # Transform the batch.\n",
    "        processed_rows = total_processed_rows + len(X_train_batch)\n",
    "        batch_X_train_count = count_vect.transform(X_train_batch[TRAINING_COLUMN])\n",
    "        rf.set_params(n_estimators=n_estimators)\n",
    "        rf.fit(batch_X_train_count, Y_train_batch)\n",
    "        rf_predictions = rf.predict(X_test_counts)\n",
    "        rf_mae = mean_absolute_error_salary_scale(Y_test, rf_predictions)\n",
    "        print('Number of rows trained on: {:10.4f}'.format(processed_rows))\n",
    "        print('Number of trees in forest: {:10.4f}'.format(n_estimators))\n",
    "        print('Random Forest Regressor Mean Absolute Error: {:10.4f}'.format(rf_mae))\n",
    "        print('')\n",
    "    \n",
    "    while total_processed_rows < X_train.shape[0] - batch_size:     \n",
    "        # Scale to get Mean Absolute Error\n",
    "        batch_index = range(total_processed_rows, total_processed_rows + batch_size)\n",
    "        X_train_batch = X_train.iloc[batch_index]\n",
    "        Y_train_batch = Y_train.iloc[batch_index]\n",
    "        run_batch(X_train_batch, Y_train_batch, n_estimators, total_processed_rows)\n",
    "        total_processed_rows += batch_size\n",
    "        n_estimators += n_estimators_per_batch\n",
    "        \n",
    "    remaining_rows = X_train.shape[0] - total_processed_rows\n",
    "    if remaining_rows == 0:\n",
    "        return\n",
    "    batch_index = range(total_processed_rows, len(X_train))\n",
    "    X_train_batch = X_train.iloc[batch_index]\n",
    "    Y_train_batch = Y_train.iloc[batch_index]\n",
    "    run_batch(X_train_batch, Y_train_batch, n_estimators, total_processed_rows)\n",
    "\n",
    "batch_processed_rf_bow(X_train_bow, Y_train, batch_size=10000, n_estimators_per_batch=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-993c9c492bf3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     n_estimators=1000, n_jobs=-1, warm_start=True)\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mrf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mrf_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    324\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrees\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m                     verbose=self.verbose, class_weight=self.class_weight)\n\u001b[0;32m--> 326\u001b[0;31m                 for i, t in enumerate(trees))\n\u001b[0m\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m             \u001b[0;31m# Collect newly grown trees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    766\u001b[0m                 \u001b[0;31m# consumption.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    769\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    717\u001b[0m                     \u001b[0mensure_ready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_managed_backend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m                     \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabort_everything\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_ready\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_ready\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    720\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    680\u001b[0m                 \u001b[0;31m# check if timeout supported in backend future implementation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m'timeout'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgetfullargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 682\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    683\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    597\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    547\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Model: Random Forests using batch learning - Tried without batch\n",
    "# learning, just ran forever.\n",
    "# Data Representation: Word2vec\n",
    "# Library used: sklearn\n",
    "\n",
    "#100/200 its per batch: 9738/9736\n",
    "\n",
    "def batch_processed_rf_bow(X_train, Y_train, batch_size=1000, n_estimators_per_batch=100):\n",
    "    \"\"\"Function to process random forest in batches using warm start.\"\"\"\n",
    "    # Define model.\n",
    "    n_estimators = n_estimators_per_batch\n",
    "    total_processed_rows = 0\n",
    "    \n",
    "    rf = sklearn.ensemble.RandomForestRegressor(\n",
    "        n_estimators=n_estimators_per_batch, n_jobs=-1, warm_start=True)\n",
    "   \n",
    "    # Reindex training and test data.\n",
    "    # Should randomize here, but we have already taken this step, so no need in our case.\n",
    "    Y_train.reset_index(drop=True)\n",
    "    \n",
    "    def run_batch(X_train_batch, Y_train_batch, n_estimators, total_processed_rows):     \n",
    "        \"\"\"Function to run next batch of random forest.\"\"\"\n",
    "        processed_rows = total_processed_rows + len(X_train_batch)\n",
    "        rf.set_params(n_estimators=n_estimators)\n",
    "        rf.fit(X_train_batch, Y_train_batch)\n",
    "        rf_predictions = rf.predict(X_test_w2v)\n",
    "        rf_mae = mae_without_log_scaling(Y_test_w2v, rf_predictions)\n",
    "        print('Number of rows trained on: {:10.4f}'.format(processed_rows))\n",
    "        print('Number of trees in forest: {:10.4f}'.format(n_estimators))\n",
    "        print('Random Forest Regressor Mean Absolute Error: {:10.4f}'.format(rf_mae))\n",
    "        print('')\n",
    "    \n",
    "    while total_processed_rows < X_train.shape[0] - batch_size:     \n",
    "        # Scale to get Mean Absolute Error\n",
    "        batch_index = range(total_processed_rows, total_processed_rows + batch_size)\n",
    "        X_train_batch = X_train.iloc[batch_index]\n",
    "        Y_train_batch = Y_train.iloc[batch_index]\n",
    "        run_batch(X_train_batch, Y_train_batch, n_estimators, total_processed_rows)\n",
    "        total_processed_rows += batch_size\n",
    "        n_estimators += n_estimators_per_batch\n",
    "        \n",
    "    remaining_rows = X_train.shape[0] - total_processed_rows\n",
    "    if remaining_rows == 0:\n",
    "        return\n",
    "    batch_index = range(total_processed_rows, len(X_train))\n",
    "    X_train_batch = X_train.iloc[batch_index]\n",
    "    Y_train_batch = Y_train.iloc[batch_index]\n",
    "    run_batch(X_train_batch, Y_train_batch, n_estimators, total_processed_rows)\n",
    "\n",
    "batch_processed_rf_bow(X_train_w2v, Y_train_w2v, batch_size=10452, n_estimators_per_batch=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interesting note: Using LDA, n_estimators = 10 performs better than 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-cc6d1601c657>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m        subsample=.8, colsample_bytree=.8, min_child_weight=.5)\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_counts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mxgb_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_counts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/rcreagh/xgboost/python-package/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, eval_set, eval_metric, early_stopping_rounds, verbose)\u001b[0m\n\u001b[1;32m    263\u001b[0m             \u001b[0mtrainDmatrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDMatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m             \u001b[0mtrainDmatrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDMatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0mevals_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/rcreagh/xgboost/python-package/xgboost/core.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, label, missing, weight, silent, feature_names, feature_types)\u001b[0m\n\u001b[1;32m    265\u001b[0m                                                      ctypes.byref(self.handle)))\n\u001b[1;32m    266\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsr_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_from_csr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsc_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_from_csc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/rcreagh/xgboost/python-package/xgboost/core.py\u001b[0m in \u001b[0;36m_init_from_csr\u001b[0;34m(self, csr)\u001b[0m\n\u001b[1;32m    292\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_void_p\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m         _check_call(_LIB.XGDMatrixCreateFromCSREx(c_array(ctypes.c_size_t, csr.indptr),\n\u001b[0;32m--> 294\u001b[0;31m                                                   \u001b[0mc_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_uint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcsr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m                                                   \u001b[0mc_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_float\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcsr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m                                                   \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_size_t\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/rcreagh/xgboost/python-package/xgboost/core.py\u001b[0m in \u001b[0;36mc_array\u001b[0;34m(ctype, values)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mc_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;34m\"\"\"Convert a python string to c array.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mctype\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Model: XGBoost\n",
    "# Data Representation: BOW\n",
    "# Library used: xgboost\n",
    "\n",
    "# n_estimators set to 1000 achieved mae: 5932\n",
    "# n_estimators set to 2000 achieved mae: 5693\n",
    "# n_estimators set to 5000 achieved mae: 5507\n",
    "\n",
    "# Model: XGBoost\n",
    "# Data Representation: TF-IDF\n",
    "# Library used: xgboost\n",
    "\n",
    "# n_estimators set to 1000 achieved mae: 6059\n",
    "# n_estimators set to 2000 achieved mae: 5827\n",
    "\n",
    "xgb = xgboost.XGBRegressor(\n",
    "       n_estimators=5000, learning_rate=.10, max_depth=10, silent=False,\n",
    "       subsample=.8, colsample_bytree=.8, min_child_weight=.5)\n",
    "\n",
    "xgb.fit(X_train_counts, Y_train)\n",
    "xgb_predictions = xgb.predict(X_test_counts)\n",
    "\n",
    "# Scale to get Mean Absolute Error\n",
    "xgb_mae = mean_absolute_error_salary_scale(Y_test, xgb_predictions)\n",
    "\n",
    "print('XGBoost Mean Absolute Error: {:10.4f}'.format(xgb_mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Mean Absolute Error:  5601.7726\n"
     ]
    }
   ],
   "source": [
    "# Model: XGBoost\n",
    "# Data Representation: BOW - normalised\n",
    "# Library used: xgboost\n",
    "# Scale - Regular Scale\n",
    "\n",
    "# n_estimators = 1000: 6074\n",
    "# n_estimators = 2000: 5817\n",
    "# n_estimators = 5601\n",
    "\n",
    "#min_child_weight=1: no improvement\n",
    "\n",
    "xgb = xgboost.XGBRegressor(\n",
    "       n_estimators=5000, learning_rate=.10, max_depth=10, silent=False,\n",
    "       subsample=.8, colsample_bytree=.8, min_child_weight=1)\n",
    "\n",
    "xgb.fit(X_train_norm, Y_train)\n",
    "xgb_predictions = xgb.predict(X_test_norm)\n",
    "\n",
    "# Scale to get Mean Absolute Error\n",
    "xgb_mae = mae_without_log_scaling(Y_test, xgb_predictions)\n",
    "\n",
    "print('XGBoost Mean Absolute Error: {:10.4f}'.format(xgb_mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Mean Absolute Error:  7010.3315\n"
     ]
    }
   ],
   "source": [
    "# Model: XGBoost\n",
    "# Data Representation: word2vec\n",
    "# Library used: xgboost\n",
    "# Scale - Regular scale\n",
    "\n",
    "# n_estimators=1000: 7020\n",
    "# n_estimators=2000: 7010\n",
    "\n",
    "xgb = xgboost.XGBRegressor(\n",
    "       n_estimators=2000, learning_rate=.10, max_depth=10,\n",
    "       subsample=.8, colsample_bytree=.8, min_child_weight=1)\n",
    "\n",
    "xgb.fit(X_train_w2v, Y_train_w2v)\n",
    "xgb_predictions = xgb.predict(X_test_w2v)\n",
    "\n",
    "joblib.dump(xgb, 'xgb_w2v.pkl')\n",
    "\n",
    "xgb_mae = mae_without_log_scaling(Y_test_w2v, xgb_predictions)\n",
    "\n",
    "print('XGBoost Mean Absolute Error: {:10.4f}'.format(xgb_mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Mean Absolute Error:  8295.2584\n"
     ]
    }
   ],
   "source": [
    "# Model: XGBoost\n",
    "# Data Representation: LDA\n",
    "# Library used: xgboost\n",
    "\n",
    "# n_estimators=5000, learning_rate=.05, min_child_weight=1: 8132\n",
    "# Tried lots of learning rates, different child weights, subsample and colsample_bytree sizes.\n",
    "# Best combination: n_estimators=5000, learning_rate=.05, max_depth=10, silent=False,\n",
    "# subsample=.8, colsample_bytree=.8, min_child_weight=1)\n",
    "\n",
    "xgb = xgboost.XGBRegressor(\n",
    "       n_estimators=5000, learning_rate=.05, max_depth=10, silent=False,\n",
    "       subsample=.8, colsample_bytree=.8, min_child_weight=1)\n",
    "\n",
    "xgb.fit(X_train_lda, Y_train_lda)\n",
    "xgb_predictions = xgb.predict(X_test_lda)\n",
    "\n",
    "# Scale to get Mean Absolute Error\n",
    "xgb_mae = mean_absolute_error_salary_scale(Y_test_lda, xgb_predictions)\n",
    "\n",
    "print('XGBoost Mean Absolute Error: {:10.4f}'.format(xgb_mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDRegressor Mean Absolute Error:  8929.7799\n"
     ]
    }
   ],
   "source": [
    "# Model: SGD\n",
    "# Data Representation: LDA\n",
    "# Library used: sklearn\n",
    "\n",
    "# Normalization not required as LDA output is normalised between 0 and\n",
    "# 1 anyway.\n",
    "\n",
    "# We want a stochastic gradient descent with l1 norm.\n",
    "sgd = sklearn.linear_model.SGDRegressor(\n",
    "    alpha=.0001, penalty='l1', n_iter=10000)\n",
    "sgd.fit(X_train_lda, Y_train)\n",
    "sgd_predictions = sgd.predict(X_test_lda)\n",
    "sgd_mae = mean_absolute_error_salary_scale(Y_test, sgd_predictions)\n",
    "print('SGDRegressor Mean Absolute Error: {:10.4f}'.format(sgd_mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120037, 10332)\n",
      "SGDRegressor Mean Absolute Error: 10040.1751\n"
     ]
    }
   ],
   "source": [
    "# Model: SGD\n",
    "# Data Representation: BOW\n",
    "# Library used: sklearn\n",
    "# Scale: Log salary scale\n",
    "\n",
    "normalizer = sklearn.preprocessing.Normalizer(norm='l1')\n",
    "X_train_norm = normalizer.fit_transform(X_train_counts.astype('float64'))\n",
    "X_test_norm = normalizer.transform(X_test_counts.astype('float64'))\n",
    "print(X_train_norm.shape)\n",
    "\n",
    "# We want a stochastic gradient descent with l1 norm.\n",
    "sgd = sklearn.linear_model.SGDRegressor(\n",
    "    alpha=.0001, penalty='l1', n_iter=10000)\n",
    "sgd.fit(X_train_norm, Y_train)\n",
    "sgd_predictions = sgd.predict(X_test_norm)\n",
    "sgd_mae = mean_absolute_error_salary_scale(Y_test, sgd_predictions)\n",
    "print('SGDRegressor Mean Absolute Error: {:10.4f}'.format(sgd_mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(119970, 10027)\n",
      "SGDRegressor Mean Absolute Error:  9408.0917\n"
     ]
    }
   ],
   "source": [
    "# Model: SGD\n",
    "# Data Representation: BOW\n",
    "# Library used: sklearn\n",
    "# Scale: Regular scale\n",
    "\n",
    "X_train_index, X_test_index, Y_train, Y_test = (\n",
    "    sklearn.model_selection.train_test_split(\n",
    "        data.index, data['SalaryNormalized'],\n",
    "        test_size=.3, random_state=42))\n",
    "\n",
    "normalizer = sklearn.preprocessing.Normalizer(norm='l1')\n",
    "X_train_norm = normalizer.fit_transform(X_train_counts.astype('float64'))\n",
    "X_test_norm = normalizer.transform(X_test_counts.astype('float64'))\n",
    "print(X_train_norm.shape)\n",
    "\n",
    "# We want a stochastic gradient descent with l1 norm.\n",
    "sgd = sklearn.linear_model.SGDRegressor(\n",
    "    alpha=.0001, penalty='l1', n_iter=1000)\n",
    "sgd.fit(X_train_norm, Y_train)\n",
    "sgd_predictions = sgd.predict(X_test_norm)\n",
    "sgd_mae = mae_without_log_scaling(Y_test, sgd_predictions)\n",
    "print('SGDRegressor Mean Absolute Error: {:10.4f}'.format(sgd_mae))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: With the same data, training in batch results in a higher error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rcreagh/xgboost/python-package/xgboost/sklearn.py:171: DeprecationWarning: The nthread parameter is deprecated as of version .6.Please use n_jobs instead.nthread is deprecated.\n",
      "  'nthread is deprecated.', DeprecationWarning)\n",
      "/Users/rcreagh/xgboost/python-package/xgboost/sklearn.py:171: DeprecationWarning: The nthread parameter is deprecated as of version .6.Please use n_jobs instead.nthread is deprecated.\n",
      "  'nthread is deprecated.', DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] max_depth=4 .....................................................\n"
     ]
    }
   ],
   "source": [
    "# XGBoost hyperparameter tuning using sklearn's Grid Search\n",
    "\n",
    "PARAMATER = 'max_depth'\n",
    "PARAMATER_VALUES = [4, 10]\n",
    "\n",
    "paramaters = {\n",
    " PARAMATER:PARAMATER_VALUES\n",
    "}\n",
    "\n",
    "loss = sklearn.metrics.make_scorer(mean_absolute_error_salary_scale,\n",
    "                    greater_is_better=False)\n",
    "\n",
    "gsearch = sklearn.model_selection.GridSearchCV(\n",
    "    estimator = XGBClassifier(\n",
    "        learning_rate =0.1, n_estimators=1000, max_depth=5,\n",
    "        min_child_weight=1.0, colsample_bytree=0.8, n_jobs=-1),\n",
    "    param_grid = paramaters, n_jobs=-1,iid=True, cv=2, scoring=loss,\n",
    "    pre_dispatch=1, verbose=10)\n",
    "\n",
    "gsearch.fit(X_train_counts, Y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(0, len(PARAMATER_VALUES)):\n",
    "    print(PARAMATER + ' ' + str(PARAMATER_VALUES[i]) + ':\\n' +\n",
    "          'Mean training absolute error: ' +\n",
    "          str(gsearch.cv_results_['mean_train_score'][i]) + ';\\n' +\n",
    "         'Mean testing absolute error: ' +\n",
    "          str(gsearch.cv_results_['mean_test_score'][i]) + ';\\n')\n",
    "\n",
    "print('Best paramater: ' + str(gsearch.best_params_))\n",
    "\n",
    "xgb_cv_predictions = gsearch.predict(X_test_counts)\n",
    "\n",
    "# Scale to get Mean Absolute Error\n",
    "xgb_cv_mae = mean_absolute_error_salary_scale(Y_test, xgb_cv_predictions)\n",
    "\n",
    "print('Mean Absolute Error for best paramater on cross validation data: {:10.4f}'.format(\n",
    "        xgb_cv_mae))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
