{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import pandas\n",
    "import sklearn.ensemble\n",
    "import sklearn.feature_extraction\n",
    "import sklearn.linear_model\n",
    "import sklearn.model_selection\n",
    "\n",
    "#import xgboost - Not working on Rob's laptop\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# General setup.\n",
    "\n",
    "# Return mean absolute error scale.\n",
    "def mean_absolute_error_salary_scale(y_test, y_predicted):\n",
    "    return sklearn.metrics.mean_absolute_error(\n",
    "        numpy.exp(y_test), numpy.exp(y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6000, 11192)\n"
     ]
    }
   ],
   "source": [
    "# Define training and test data.\n",
    "\n",
    "data = pandas.read_csv('data/train_large.csv')\n",
    "\n",
    "X_train_index, X_test_index, Y_train, Y_test = sklearn.model_selection.train_test_split(\n",
    "    data.index, data['LogSalaryNormalized'], test_size=.3, random_state=42)\n",
    "\n",
    "# Define LDA data.\n",
    "lda_data = pandas.read_csv('data/train_large_lda_50.csv')\n",
    "\n",
    "X_train_lda = lda_data.iloc[X_train_index]\n",
    "X_test_lda = lda_data.iloc[X_test_index]\n",
    "\n",
    "# Define BOW data.\n",
    "TRAINING_COLUMN = 'FullDescriptionWithTitle'\n",
    "\n",
    "X_train_bow = data.iloc[X_train_index]\n",
    "X_test_bow = data.iloc[X_test_index]\n",
    "\n",
    "count_vect = sklearn.feature_extraction.text.CountVectorizer(\n",
    "    stop_words='english', min_df=5, decode_error='ignore')\n",
    "\n",
    "# Transform BOW test set.\n",
    "X_train_counts = count_vect.fit_transform(\n",
    "    X_train_bow[TRAINING_COLUMN])\n",
    "X_test_counts = count_vect.transform(\n",
    "    X_test_bow[TRAINING_COLUMN])\n",
    "\n",
    "print(X_test_counts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Regressor Mean Absolute Error:  8380.5428\n"
     ]
    }
   ],
   "source": [
    "# Model: Random Forests\n",
    "# Data Representation: LDA\n",
    "# Library used: sklearn\n",
    "\n",
    "# NOTE: Have since retrained LDA using sklearn and online mini-batch learning.\n",
    "\n",
    "rf = sklearn.ensemble.RandomForestRegressor(\n",
    "    n_estimators=1000, n_jobs=-1, warm_start=True)\n",
    "\n",
    "rf.fit(X_train_lda, Y_train)\n",
    "rf_predictions = rf.predict(X_test_lda)\n",
    "\n",
    "# Scale to get Mean Absolute Error\n",
    "rf_mae = mean_absolute_error_salary_scale(Y_test, rf_predictions)\n",
    "\n",
    "print('Random Forest Regressor Mean Absolute Error: {:10.4f}'.format(rf_mae))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interesting note: Using LDA, n_estimators = 10 performs better than 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model: XGBoost\n",
    "# Data Representation: LDA\n",
    "# Library used: xgboost\n",
    "\n",
    "# NOTE: xgboost will not import on Rob's laptop.\n",
    "\n",
    "xgb = xgboost.XGBRegressor(\n",
    "        learning_rate=.10, max_depth=10, n_estimators=1000, silent=False,\n",
    "        subsample=.8, colsample_bytree=.8, nthread=4, min_child_weight=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDRegressor Mean Absolute Error:  8929.7799\n"
     ]
    }
   ],
   "source": [
    "# Model: SGD\n",
    "# Data Representation: LDA\n",
    "# Library used: sklearn\n",
    "\n",
    "# Normalization not required as LDA output is normalised between 0 and\n",
    "# 1 anyway.\n",
    "\n",
    "# We want a stochastic gradient descent with l1 norm.\n",
    "sgd = sklearn.linear_model.SGDRegressor(\n",
    "    alpha=.0001, penalty='l1', n_iter=10000)\n",
    "sgd.fit(X_train_lda, Y_train)\n",
    "sgd_predictions = sgd.predict(X_test_lda)\n",
    "sgd_mae = mean_absolute_error_salary_scale(Y_test, sgd_predictions)\n",
    "print('SGDRegressor Mean Absolute Error: {:10.4f}'.format(sgd_mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Model: SGD\n",
    "# Data Representation: BOW\n",
    "# Library used: sklearn\n",
    "\n",
    "normalizer = sklearn.preprocessing.Normalizer(norm='l1')\n",
    "X_train_norm = normalizer.fit_transform(X_train_counts.astype('float64'))\n",
    "X_test_norm = normalizer.transform(X_test_counts.astype('float64'))\n",
    "\n",
    "# We want a stochastic gradient descent with l1 norm.\n",
    "sgd = sklearn.linear_model.SGDRegressor(\n",
    "    alpha=.0001, penalty='l1', n_iter=10000)\n",
    "sgd.fit(X_train_lda, Y_train)\n",
    "sgd_predictions = sgd.predict(X_test_lda)\n",
    "sgd_mae = mean_absolute_error_salary_scale(Y_test, sgd_predictions)\n",
    "print('SGDRegressor Mean Absolute Error: {:10.4f}'.format(sgd_mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows trained on:  1000.0000\n",
      "Number of trees in forest:   100.0000\n",
      "Random Forest Regressor Mean Absolute Error:  9446.9799\n",
      "\n",
      "Number of rows trained on:  2000.0000\n",
      "Number of trees in forest:   200.0000\n",
      "Random Forest Regressor Mean Absolute Error:  9244.8231\n",
      "\n",
      "Number of rows trained on:  3000.0000\n",
      "Number of trees in forest:   300.0000\n",
      "Random Forest Regressor Mean Absolute Error:  9130.0379\n",
      "\n",
      "Number of rows trained on:  4000.0000\n",
      "Number of trees in forest:   400.0000\n",
      "Random Forest Regressor Mean Absolute Error:  9102.5227\n",
      "\n",
      "Number of rows trained on:  5000.0000\n",
      "Number of trees in forest:   500.0000\n",
      "Random Forest Regressor Mean Absolute Error:  9106.9557\n",
      "\n",
      "Number of rows trained on:  6000.0000\n",
      "Number of trees in forest:   600.0000\n",
      "Random Forest Regressor Mean Absolute Error:  9113.0142\n",
      "\n",
      "Number of rows trained on:  7000.0000\n",
      "Number of trees in forest:   700.0000\n",
      "Random Forest Regressor Mean Absolute Error:  9102.6697\n",
      "\n",
      "Number of rows trained on:  8000.0000\n",
      "Number of trees in forest:   800.0000\n",
      "Random Forest Regressor Mean Absolute Error:  9104.6107\n",
      "\n",
      "Number of rows trained on:  9000.0000\n",
      "Number of trees in forest:   900.0000\n",
      "Random Forest Regressor Mean Absolute Error:  9109.8692\n",
      "\n",
      "Number of rows trained on: 10000.0000\n",
      "Number of trees in forest:  1000.0000\n",
      "Random Forest Regressor Mean Absolute Error:  9105.5109\n",
      "\n",
      "Number of rows trained on: 11000.0000\n",
      "Number of trees in forest:  1100.0000\n",
      "Random Forest Regressor Mean Absolute Error:  9104.5456\n",
      "\n",
      "Number of rows trained on: 12000.0000\n",
      "Number of trees in forest:  1200.0000\n",
      "Random Forest Regressor Mean Absolute Error:  9113.8227\n",
      "\n",
      "Number of rows trained on: 13000.0000\n",
      "Number of trees in forest:  1300.0000\n",
      "Random Forest Regressor Mean Absolute Error:  9112.1865\n",
      "\n",
      "Number of rows trained on: 14000.0000\n",
      "Number of trees in forest:  1400.0000\n",
      "Random Forest Regressor Mean Absolute Error:  9108.4094\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Model: Random Forests using batch learning\n",
    "# Data Representation: LDA\n",
    "# Library used: sklearn\n",
    "\n",
    "def batch_processed_rf(X_train, Y_train, batch_size=1000, n_estimators_per_batch=100):\n",
    "    \"\"\"Function to process random forest in batches using warm start.\"\"\"\n",
    "    \n",
    "    def run_batch(X_train_batch, Y_train_batch, n_estimators, total_processed_rows):     \n",
    "        \"\"\"Function to run next batch of random forest.\"\"\"\n",
    "        processed_rows = total_processed_rows + len(X_train_batch)\n",
    "        rf.set_params(n_estimators=n_estimators)\n",
    "        rf.fit(X_train_batch, Y_train_batch)\n",
    "        rf_predictions = rf.predict(X_test_lda)\n",
    "        rf_mae = mean_absolute_error_salary_scale(Y_test, rf_predictions)\n",
    "        print('Number of rows trained on: {:10.4f}'.format(processed_rows))\n",
    "        print('Number of trees in forest: {:10.4f}'.format(n_estimators))\n",
    "        print('Random Forest Regressor Mean Absolute Error: {:10.4f}'.format(rf_mae))\n",
    "        print('')\n",
    "    \n",
    "    n_estimators = n_estimators_per_batch\n",
    "    total_processed_rows = 0\n",
    "    \n",
    "    # Define model.\n",
    "    rf = sklearn.ensemble.RandomForestRegressor(\n",
    "    n_estimators=n_estimators_per_batch, n_jobs=-1, warm_start=True)\n",
    "    \n",
    "    # Reindex training and test data.\n",
    "    # Should randomize here, but we have already taken this step, so no need in our case.\n",
    "    X_train.reset_index(drop=True)\n",
    "    Y_train.reset_index(drop=True)\n",
    "    \n",
    "    while total_processed_rows < len(X_train) - batch_size:     \n",
    "        # Scale to get Mean Absolute Error\n",
    "        batch_index = range(total_processed_rows, total_processed_rows + batch_size)\n",
    "        X_train_batch = X_train.iloc[batch_index]\n",
    "        Y_train_batch = Y_train.iloc[batch_index]\n",
    "        run_batch(X_train_batch, Y_train_batch, n_estimators, total_processed_rows)\n",
    "        total_processed_rows += batch_size\n",
    "        n_estimators += n_estimators_per_batch\n",
    "        \n",
    "    remaining_rows = len(X_train) - total_processed_rows\n",
    "    if remaining_rows == 0:\n",
    "        return\n",
    "    batch_index = range(total_processed_rows, len(X_train))\n",
    "    X_train_batch = X_train.iloc[batch_index]\n",
    "    Y_train_batch = Y_train.iloc[batch_index]\n",
    "    run_batch(X_train_batch, Y_train_batch, n_estimators, total_processed_rows)\n",
    "\n",
    "batch_processed_rf(X_train_lda, Y_train, batch_size=1000, n_estimators_per_batch=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: With the same data, training in batch results in a higher error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows trained on:  1000.0000\n",
      "Number of trees in forest:   100.0000\n",
      "Random Forest Regressor Mean Absolute Error:  9208.7681\n",
      "\n",
      "Number of rows trained on:  2000.0000\n",
      "Number of trees in forest:   200.0000\n",
      "Random Forest Regressor Mean Absolute Error:  9121.8690\n",
      "\n",
      "Number of rows trained on:  3000.0000\n",
      "Number of trees in forest:   300.0000\n",
      "Random Forest Regressor Mean Absolute Error:  9064.8266\n",
      "\n",
      "Number of rows trained on:  4000.0000\n",
      "Number of trees in forest:   400.0000\n",
      "Random Forest Regressor Mean Absolute Error:  9042.4756\n",
      "\n",
      "Number of rows trained on:  5000.0000\n",
      "Number of trees in forest:   500.0000\n",
      "Random Forest Regressor Mean Absolute Error:  8994.1700\n",
      "\n",
      "Number of rows trained on:  6000.0000\n",
      "Number of trees in forest:   600.0000\n",
      "Random Forest Regressor Mean Absolute Error:  9027.7942\n",
      "\n",
      "Number of rows trained on:  7000.0000\n",
      "Number of trees in forest:   700.0000\n",
      "Random Forest Regressor Mean Absolute Error:  9017.5897\n",
      "\n",
      "Number of rows trained on:  8000.0000\n",
      "Number of trees in forest:   800.0000\n",
      "Random Forest Regressor Mean Absolute Error:  9006.1406\n",
      "\n",
      "Number of rows trained on:  9000.0000\n",
      "Number of trees in forest:   900.0000\n",
      "Random Forest Regressor Mean Absolute Error:  9016.1419\n",
      "\n",
      "Number of rows trained on: 10000.0000\n",
      "Number of trees in forest:  1000.0000\n",
      "Random Forest Regressor Mean Absolute Error:  8999.7377\n",
      "\n",
      "Number of rows trained on: 11000.0000\n",
      "Number of trees in forest:  1100.0000\n",
      "Random Forest Regressor Mean Absolute Error:  8999.8191\n",
      "\n",
      "Number of rows trained on: 12000.0000\n",
      "Number of trees in forest:  1200.0000\n",
      "Random Forest Regressor Mean Absolute Error:  9006.1123\n",
      "\n",
      "Number of rows trained on: 13000.0000\n",
      "Number of trees in forest:  1300.0000\n",
      "Random Forest Regressor Mean Absolute Error:  9008.8617\n",
      "\n",
      "Number of rows trained on: 14000.0000\n",
      "Number of trees in forest:  1400.0000\n",
      "Random Forest Regressor Mean Absolute Error:  9002.6250\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Model: Random Forests using batch learning\n",
    "# Data Representation: Bag of Words\n",
    "# Library used: sklearn\n",
    "\n",
    "# NOTE: This is just to get this set up and working.\n",
    "# TODO: Move this to be a single function under 'Random Forests' where you simply define the\n",
    "# model as being BOW or LDA.\n",
    "\n",
    "def batch_processed_rf_bow(X_train, Y_train, batch_size=1000, n_estimators_per_batch=100):\n",
    "    \"\"\"Function to process random forest in batches using warm start.\"\"\"\n",
    "    # Define model.\n",
    "    n_estimators = n_estimators_per_batch\n",
    "    total_processed_rows = 0\n",
    "    \n",
    "    rf = sklearn.ensemble.RandomForestRegressor(\n",
    "        n_estimators=n_estimators_per_batch, n_jobs=-1, warm_start=True)\n",
    "   \n",
    "    # Reindex training and test data.\n",
    "    # Should randomize here, but we have already taken this step, so no need in our case.\n",
    "    Y_train.reset_index(drop=True)\n",
    "    \n",
    "    def run_batch(X_train_batch, Y_train_batch, n_estimators, total_processed_rows):     \n",
    "        \"\"\"Function to run next batch of random forest.\"\"\"\n",
    "        # Transform the batch.\n",
    "        processed_rows = total_processed_rows + len(X_train_batch)\n",
    "        batch_X_train_count = count_vect.transform(X_train_batch[TRAINING_COLUMN])\n",
    "        rf.set_params(n_estimators=n_estimators)\n",
    "        rf.fit(batch_X_train_count, Y_train_batch)\n",
    "        rf_predictions = rf.predict(X_test_counts)\n",
    "        rf_mae = mean_absolute_error_salary_scale(Y_test, rf_predictions)\n",
    "        print('Number of rows trained on: {:10.4f}'.format(processed_rows))\n",
    "        print('Number of trees in forest: {:10.4f}'.format(n_estimators))\n",
    "        print('Random Forest Regressor Mean Absolute Error: {:10.4f}'.format(rf_mae))\n",
    "        print('')\n",
    "    \n",
    "    while total_processed_rows < X_train.shape[0] - batch_size:     \n",
    "        # Scale to get Mean Absolute Error\n",
    "        batch_index = range(total_processed_rows, total_processed_rows + batch_size)\n",
    "        X_train_batch = X_train.iloc[batch_index]\n",
    "        Y_train_batch = Y_train.iloc[batch_index]\n",
    "        run_batch(X_train_batch, Y_train_batch, n_estimators, total_processed_rows)\n",
    "        total_processed_rows += batch_size\n",
    "        n_estimators += n_estimators_per_batch\n",
    "        \n",
    "    remaining_rows = X_train.shape[0] - total_processed_rows\n",
    "    if remaining_rows == 0:\n",
    "        return\n",
    "    batch_index = range(total_processed_rows, len(X_train))\n",
    "    X_train_batch = X_train.iloc[batch_index]\n",
    "    Y_train_batch = Y_train.iloc[batch_index]\n",
    "    run_batch(X_train_batch, Y_train_batch, n_estimators, total_processed_rows)\n",
    "\n",
    "batch_processed_rf_bow(X_train_bow, Y_train, batch_size=1000, n_estimators_per_batch=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
