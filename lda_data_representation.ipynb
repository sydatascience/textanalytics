{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Using LDA package\n",
    "# https://pypi.python.org/pypi/lda\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "import numpy as np\n",
    "import pandas\n",
    "import preprocess_text_data\n",
    "import sklearn.decomposition\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "INPUT_FILE = 'train.csv'\n",
    "OUTPUT_FILE = '%s_lda_50.csv' % INPUT_FILE.split(\".\")[0]\n",
    "MIN_WORD_COUNT = 50\n",
    "\n",
    "X, vocab = preprocess_text_data.get_lda_training_data(\n",
    "    INPUT_FILE, 'CompleteJobListing', MIN_WORD_COUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lda_model.pkl']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If I do batch learning rather than mini batch, I run out of RAM.\n",
    "\n",
    "model = sklearn.decomposition.LatentDirichletAllocation(\n",
    "    n_topics=30, max_iter=50, random_state=1, learning_method='online',\n",
    "    batch_size=16384, n_jobs=-1)\n",
    "\n",
    "model.fit_transform(X)\n",
    "\n",
    "joblib.dump(model, 'lda_model.pkl') \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topic_word = model.components_\n",
    "n_top_words = 15\n",
    "\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "  topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "  print('Topic {}: {}'.format(i, ' '.join(topic_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interesting note: Mini-Batch learning model with far fewer max_its takes far longer to train than learning without this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is, our 50 dimensional representation of the original data.\n",
    "\n",
    "df = pandas.DataFrame(topic_word)\n",
    "df.to_csv('data/%s' % OUTPUT_FILE, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110404.92068\n"
     ]
    }
   ],
   "source": [
    "topic_word = model.components_\n",
    "df = pandas.DataFrame(topic_word)\n",
    "print(max(df.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
