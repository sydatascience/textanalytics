{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Core python libraries\n",
    "import csv\n",
    "import re\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "\n",
    "# External libraries.\n",
    "import nltk\n",
    "import numpy\n",
    "import pandas\n",
    "import scipy\n",
    "import sklearn\n",
    "import tempfile\n",
    "\n",
    "# Tensorflow and related.\n",
    "import tensorflow as tf\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tf_utils\n",
    "\n",
    "# So you know when this code block finishes.\n",
    "print (\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "171482\n"
     ]
    }
   ],
   "source": [
    "#TODO(Max): make a library that does all this preprocessing\n",
    "data = pandas.read_csv(\"data/train.csv\") # Can read a subset. First nrows of the total.\n",
    "LABEL_COLUMN = \"SalaryNormalized\"\n",
    "\n",
    "TEST_SIZE = .3\n",
    "\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# sklearn.cross_validation has been replaced with model_selection.\n",
    "X_train_index, X_test_index, Y_train, Y_test = sklearn.model_selection.train_test_split(\n",
    "    data.index, data[LABEL_COLUMN], test_size=TEST_SIZE, random_state=42)\n",
    "\n",
    "# Keep train and test as pandas dataframes.\n",
    "X_train = data.iloc[X_train_index]\n",
    "X_test = data.iloc[X_test_index]\n",
    "\n",
    "y_train = numpy.array(X_train[LABEL_COLUMN].astype(numpy.float32))\n",
    "y_test= numpy.array(X_test[LABEL_COLUMN].astype(numpy.float32))\n",
    "\n",
    "print (\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Normalizing input values to mean 0, standard deviation 1.\n",
    "train_mean, train_std = y_train.mean(), y_train.std()\n",
    "\n",
    "# Normalize input labels and expectations\n",
    "y_train = tf_utils.normalize_input(y_train, train_mean, train_std)\n",
    "y_test = tf_utils.normalize_input(y_test, train_mean, train_std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17698.3\n",
      "[ 0.89645004 -0.66550416  3.22435188 ..., -0.29010177  0.89645004\n",
      "  0.95295256]\n"
     ]
    }
   ],
   "source": [
    "print(train_std)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target_column = \"CompleteJobListingStemmed\"\n",
    "\n",
    "x_train_text = X_train[target_column]\n",
    "x_test_text = X_test[target_column]\n",
    "\n",
    "max_document_length = max([len(x.split(\" \")) for x in x_train_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: 120037, 1368\n",
      "x_test shape: 51445, 1368\n",
      "Total distinct words: 8712\n"
     ]
    }
   ],
   "source": [
    "# Look here for a tutorial https://medium.com/@ilblackdragon/tensorflow-text-classification-615198df9231\n",
    "min_word_frequency = 75\n",
    "\n",
    "vocab_processor = tf.contrib.learn.preprocessing.VocabularyProcessor(\n",
    "    max_document_length, min_frequency=min_word_frequency)\n",
    "x_train = numpy.array(list(vocab_processor.fit_transform(x_train_text)))\n",
    "print(\"x_train shape: %s, %s\" % x_train.shape)\n",
    "x_test = numpy.array(list(vocab_processor.transform(x_test_text)))\n",
    "print(\"x_test shape: %s, %s\" % x_test.shape)\n",
    "n_words = len(vocab_processor.vocabulary_)\n",
    "print(\"Total distinct words: %d\" % n_words)\n",
    "\n",
    "# Try looking at https://github.com/nfmcclure/tensorflow_cookbook/blob/master/07_Natural_Language_Processing/02_Working_with_Bag_of_Words/02_bag_of_words.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120037\n"
     ]
    }
   ],
   "source": [
    "print(len(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define data nput functions. These have to be done here as the \n",
    "# functions take no arguments. Also the dict labels are specific to \n",
    "# the model.\n",
    "def input_fn():\n",
    "    for batch_input, batch_labels in tf_utils.generate_batch(\n",
    "        batch_size, number_of_batches, max_document_length,\n",
    "                                                    x_train, y_train):\n",
    "        yield {x_data: batch_input, y_target: batch_labels}\n",
    "\n",
    "def eval_input_fn():\n",
    "    for batch_input, batch_labels in tf_utils.generate_batch(\n",
    "        batch_size, number_of_validation_batches, max_document_length,\n",
    "                                                    x_test, y_test):\n",
    "        yield {x_data: batch_input, y_target: batch_labels}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Woohoo this looks like my bag of words shape :D (256, 8712)\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n"
     ]
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "\n",
    "sess = tf.Session(graph=graph)\n",
    "\n",
    "learning_rate = .01\n",
    "\n",
    "l1_regularization_coef = 0.0\n",
    "l2_regularization_coef = 0.001\n",
    "\n",
    "# if not RMSE then MAE.\n",
    "HUBER = False\n",
    "RMSE = True\n",
    "\n",
    "#Implement this by batch.\n",
    "batch_size = 256\n",
    "\n",
    "#Size of input dictionary. Number of distinct words.\n",
    "vocab_size = len(vocab_processor.vocabulary_)\n",
    "\n",
    "def huber_loss(labels, predictions, delta=1.0):\n",
    " residual = tf.abs(predictions - labels)\n",
    " condition = tf.less(residual, delta)\n",
    " small_res = 0.5 * tf.square(residual)\n",
    " large_res = delta * residual - 0.5 * tf.square(delta)\n",
    " return tf.reduce_mean(tf.where(condition, small_res, large_res))\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    with tf.name_scope(\"data\"):\n",
    "        # Initialize data placeholders\n",
    "        x_data = tf.placeholder(shape=[batch_size, max_document_length], dtype=tf.int32)\n",
    "        y_target = tf.placeholder(shape=[batch_size, 1], dtype=tf.float32, name=\"labels\")\n",
    "\n",
    "    with tf.name_scope(\"bag_of_words\"):\n",
    "        with tf.device('/cpu:0'):\n",
    "            identity_mat = tf.diag(tf.ones(shape=[vocab_size]))\n",
    "            # This is also an embedding_lookup_sparse function that would create a sparse tensor.\n",
    "            x_embed = tf.nn.embedding_lookup(identity_mat, x_data)\n",
    "            # Collapse across the document length dimension. Leaving only the number of tokens.\n",
    "            bag_of_words = tf.convert_to_tensor_or_sparse_tensor(tf.reduce_sum(x_embed, 1))\n",
    "\n",
    "            print(\"Woohoo this looks like my bag of words shape :D %s\" % bag_of_words.shape)\n",
    "\n",
    "    # Create variables for regression\n",
    "    A = tf.Variable(tf.zeros(shape=[vocab_size, 1]))\n",
    "    b = tf.Variable(tf.zeros(shape=[1]))\n",
    "\n",
    "    # A = tf.Variable(tf.random_normal(shape=[vocab_size, 1], stddev=.05))\n",
    "    # Intercept term. A scalar.\n",
    "    # b = tf.Variable(tf.random_normal(shape=[1], stddev=.05))\n",
    "\n",
    "    # Model output\n",
    "    product = tf.matmul(bag_of_words, A)\n",
    "    model_output = tf.add(product, b, name=\"predictions\")\n",
    "\n",
    "    with tf.name_scope(\"loss\"):\n",
    "\n",
    "        # TODO(Max) try root mean square error. Should this be merely mean square error?\n",
    "        if HUBER:\n",
    "            loss = huber_loss(y_target, model_output)\n",
    "        elif RMSE:\n",
    "            loss = tf.sqrt(tf.reduce_mean(tf.pow(tf.subtract(y_target, model_output), 2)))\n",
    "        else:\n",
    "            # Mean Absolute Error.\n",
    "            loss = tf.reduce_mean(tf.abs(tf.subtract(y_target, model_output)))\n",
    "\n",
    "        # If we are using regularization.\n",
    "        if l1_regularization_coef > 0 or l2_regularization_coef > 0:\n",
    "            l1_regularizer = tf.contrib.layers.l1_regularizer(scale=l1_regularization_coef)\n",
    "            l2_regularizer = tf.contrib.layers.l2_regularizer(scale=l2_regularization_coef)\n",
    "\n",
    "            weights = tf.trainable_variables() # all vars of your graph\n",
    "            regularization_penalty = tf.add(tf.contrib.layers.apply_regularization(l1_regularizer, weights),\n",
    "                                             tf.contrib.layers.apply_regularization(l2_regularizer, weights))\n",
    "            loss = loss + regularization_penalty # this loss needs to be minimized\n",
    "\n",
    "    with tf.name_scope(\"reporting\"):\n",
    "        # Converts back to original, salary scale.\n",
    "        error_salary_scale = tf.multiply(tf.subtract(y_target, model_output), train_std)\n",
    "\n",
    "        mean_absolute_error_salary_scale = tf.reduce_mean(\n",
    "            tf.abs(error_salary_scale))\n",
    "\n",
    "        # Log for tensorboard\n",
    "        training_summary = tf.summary.scalar('train_loss', loss)\n",
    "        validation_summary = tf.summary.scalar('validation_loss', loss)\n",
    "        # Add mean absolute errors.\n",
    "        training_mae = tf.summary.scalar('train_mae', mean_absolute_error_salary_scale)\n",
    "        validation_mae = tf.summary.scalar('validation_mae', mean_absolute_error_salary_scale)\n",
    "\n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "\n",
    "    # Declare optimizer\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "\n",
    "    \n",
    "    # This one seems to work better :D\n",
    "    train_op = tf.contrib.layers.optimize_loss(      \n",
    "     loss, tf.contrib.framework.get_global_step(),      \n",
    "     optimizer='Adam'\n",
    "        , learning_rate=learning_rate, clip_gradients=1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of\n",
    "number_of_epochs = 20\n",
    "batches_per_epoch = len(x_train_text)/batch_size\n",
    "number_of_batches = math.ceil(len(x_train_text)/batch_size)\n",
    "number_of_validation_batches = number_of_batches\n",
    "\n",
    "logs_dir = \"logs/bagofwords\"\n",
    "\n",
    "checkpoint_frequency = 50\n",
    "\n",
    "reporting_frequency = 3\n",
    "\n",
    "total_loss = 0\n",
    "validation_loss = 0\n",
    "validation_batch_average_window = 100\n",
    "\n",
    "ckpt = None\n",
    "ckpt = tf.train.get_checkpoint_state(logs_dir)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Where to save this model.\n",
    "model_path = os.path.join(logs_dir, time.strftime(\"%Y-%m-%d-%H-%M-%S\"))\n",
    "                    \n",
    "if ckpt:\n",
    "    restore_from_model = True\n",
    "    print(\"Restore from model!\")\n",
    "else:\n",
    "    restore_from_model = False\n",
    "\n",
    "with tf.Session(graph=graph, config=tf.ConfigProto(\n",
    "    allow_soft_placement=True, log_device_placement=True)) as session:\n",
    "    summaries = tf.summary.merge_all()                                                       \n",
    "    writer = tf.summary.FileWriter(model_path,\n",
    "        graph=session.graph)                  \n",
    "\n",
    "    saver = tf.train.Saver(tf.global_variables()) \n",
    "                                                                                                  \n",
    "    session.run(tf.global_variables_initializer())\n",
    "    \n",
    "    \n",
    "    if restore_from_model:\n",
    "        saver.restore(session, ckpt.model_checkpoint_path)\n",
    "        \n",
    "    global_step = 0\n",
    "        \n",
    "    for epoch in range(number_of_epochs):\n",
    "        eval_feed_generator = eval_input_fn()\n",
    "        # Shuffle the training input\n",
    "        x_train, y_train = tf_utils.unison_shuffled_copies(x_train, y_train)\n",
    "\n",
    "        for step, feed_dict in enumerate(input_fn()):\n",
    "            global_step += 1\n",
    "            start_time = time.time()\n",
    "\n",
    "            _, temp_loss = session.run([train_op, loss], feed_dict=feed_dict)\n",
    "            \n",
    "            duration = time.time() - start_time\n",
    "            \n",
    "            print(\"Epoch: %s Iteration: %s : Loss: %.4f (%.3f sec)\" % \n",
    "                  (epoch, (step + 1), temp_loss, duration))\n",
    "\n",
    "            train_summ, train_mae = session.run([training_summary, training_mae], feed_dict=feed_dict)\n",
    "            writer.add_summary(train_summ, global_step)\n",
    "            writer.add_summary(train_mae, global_step)\n",
    "            _, valid_summ, valid_mae = session.run(\n",
    "                    [loss, validation_summary, validation_mae],\n",
    "                    feed_dict=next(eval_feed_generator))\n",
    "            writer.add_summary(valid_summ, global_step)\n",
    "            writer.add_summary(valid_mae, global_step)\n",
    "                \n",
    "            writer.flush()                \n",
    "\n",
    "            if (global_step % checkpoint_frequency == 0) or (global_step == number_of_batches):\n",
    "                print(\"Saving at epoch %s step: %s\" % (epoch, step + 1))\n",
    "                saver.save(session, model_path, global_step=tf.contrib.framework.get_global_step())\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from logs/bagofwords/2017-06-16-22-07-21-6995\n",
      "Loss over first 256 observations = 19806.4570312\n",
      "Loss over first 512 observations = 19747.9160156\n",
      "Loss over first 768 observations = 19691.8925781\n",
      "Loss over first 1024 observations = 19551.6015625\n",
      "Loss over first 1280 observations = 19372.3335938\n",
      "Loss over first 1536 observations = 19404.9082031\n",
      "Loss over first 1792 observations = 19402.4319196\n",
      "Loss over first 2048 observations = 19451.6975098\n",
      "Loss over first 2304 observations = 19508.781901\n",
      "Loss over first 2560 observations = 19445.9443359\n",
      "Loss over first 2816 observations = 19390.5348011\n",
      "Loss over first 3072 observations = 19401.4513346\n",
      "Loss over first 3328 observations = 19324.3898738\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-13a3bae39878>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_feed_dict\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_input_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         valid_loss, mae = session.run(\n\u001b[0;32m---> 21\u001b[0;31m             [loss, mean_absolute_error_salary_scale], feed_dict=eval_feed_dict)\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0;31m#print(\"Batch: %d Mean: %.4f\" % (step, mae))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mmae\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# How much data to evaluate\n",
    "#number_of_batches = len(x_train_text)/batch_size\n",
    "number_of_validation_batches = math.floor(len(x_test_text)/batch_size)\n",
    "ckpt = tf.train.get_checkpoint_state(logs_dir)                                     \n",
    "\n",
    "#Something is totally off here.\n",
    "\n",
    "# Write TF Model Evaluation Code here. \n",
    "with tf.Session(graph=graph, config=tf.ConfigProto(\n",
    "    allow_soft_placement=True, log_device_placement=True)) as session:\n",
    "    \n",
    "    saver = tf.train.Saver(tf.global_variables()) \n",
    "                                                                                                  \n",
    "    session.run(tf.global_variables_initializer())\n",
    "    \n",
    "    saver.restore(session, ckpt.model_checkpoint_path)\n",
    "\n",
    "    total_loss = 0\n",
    "    for step, eval_feed_dict in enumerate(eval_input_fn()):    \n",
    "        valid_loss, mae = session.run(\n",
    "            [loss, mean_absolute_error_salary_scale], feed_dict=eval_feed_dict)\n",
    "        #print(\"Batch: %d Mean: %.4f\" % (step, mae))\n",
    "        total_loss += mae\n",
    "        print (\"Loss over first %s observations = %s\" % ((step + 1) * batch_size, total_loss / (step + 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Attempted to use a closed Session.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-75-72254d444db9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    912\u001b[0m     \u001b[0;31m# Check session.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_closed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 914\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Attempted to use a closed Session.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    915\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       raise RuntimeError('The Session graph is empty.  Add operations to the '\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Attempted to use a closed Session."
     ]
    }
   ],
   "source": [
    "#model_output.eval()\n",
    "eval_generator = eval_input_fn()\n",
    "\n",
    "with sess:\n",
    "    saver = tf.train.Saver(tf.global_variables())                                                                                               \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(model_output, feed_dict=next(eval_generator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/util/deprecation.py:335: calling LinearRegressor.predict (from tensorflow.contrib.learn.python.learn.estimators.linear) with outputs=None is deprecated and will be removed after 2017-03-01.\n",
      "Instructions for updating:\n",
      "Please switch to predict_scores, or set `outputs` argument.\n"
     ]
    },
    {
     "ename": "NotFittedError",
     "evalue": "Couldn't find trained model at /tmp/tmpc4vvis3j.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-114-923d676c9ea3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mprint_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-114-923d676c9ea3>\u001b[0m in \u001b[0;36mprint_results\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprint_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0my_vals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mx_vals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mLABEL_COLUMN\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    333\u001b[0m               \u001b[0m_call_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorator_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_qualified_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m               func.__module__, arg_name, arg_value, date, instructions)\n\u001b[0;32m--> 335\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m     new_func.__doc__ = _add_deprecated_arg_notice_to_docstring(\n\u001b[1;32m    337\u001b[0m         func.__doc__, date, instructions)\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    333\u001b[0m               \u001b[0m_call_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorator_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_qualified_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m               func.__module__, arg_name, arg_value, date, instructions)\n\u001b[0;32m--> 335\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m     new_func.__doc__ = _add_deprecated_arg_notice_to_docstring(\n\u001b[1;32m    337\u001b[0m         func.__doc__, date, instructions)\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/estimators/linear.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, input_fn, batch_size, outputs, as_iterable)\u001b[0m\n\u001b[1;32m    755\u001b[0m           \u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m           as_iterable=as_iterable)\n\u001b[0m\u001b[1;32m    758\u001b[0m     return super(LinearRegressor, self).predict(\n\u001b[1;32m    759\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    333\u001b[0m               \u001b[0m_call_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorator_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_qualified_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m               func.__module__, arg_name, arg_value, date, instructions)\n\u001b[0;32m--> 335\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m     new_func.__doc__ = _add_deprecated_arg_notice_to_docstring(\n\u001b[1;32m    337\u001b[0m         func.__doc__, date, instructions)\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/estimators/linear.py\u001b[0m in \u001b[0;36mpredict_scores\u001b[0;34m(self, x, input_fn, batch_size, as_iterable)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 792\u001b[0;31m         as_iterable=as_iterable)\n\u001b[0m\u001b[1;32m    793\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mas_iterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_as_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m             \u001b[0m_call_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorator_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_qualified_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m             func.__module__, arg_name, date, instructions)\n\u001b[0;32m--> 281\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m     new_func.__doc__ = _add_deprecated_arg_notice_to_docstring(\n\u001b[1;32m    283\u001b[0m         func.__doc__, date, instructions)\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, input_fn, batch_size, outputs, as_iterable)\u001b[0m\n\u001b[1;32m    563\u001b[0m         \u001b[0mfeed_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 565\u001b[0;31m         as_iterable=as_iterable)\n\u001b[0m\u001b[1;32m    566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mget_variable_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\u001b[0m in \u001b[0;36m_infer_model\u001b[0;34m(self, input_fn, feed_fn, outputs, as_iterable, iterate_batches)\u001b[0m\n\u001b[1;32m    849\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m       raise NotFittedError(\"Couldn't find trained model at %s.\"\n\u001b[0;32m--> 851\u001b[0;31m                            % self._model_dir)\n\u001b[0m\u001b[1;32m    852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFittedError\u001b[0m: Couldn't find trained model at /tmp/tmpc4vvis3j."
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "def print_results(model):\n",
    "    y_vals = [value for value in model.predict(input_fn=eval_input_fn)]\n",
    "    x_vals = X_test[LABEL_COLUMN]\n",
    "\n",
    "\n",
    "    plt.scatter(x_vals, y_vals)\n",
    "    plt.show()\n",
    "\n",
    "    print(x_vals-y_vals)\n",
    "    #print(sorted([set(y_vals)]))\n",
    "\n",
    "    print(\"Mean absolute error: %s\" % \n",
    "          mean_absolute_error_salary_scale(y_vals, x_vals))\n",
    "    \n",
    "    \n",
    "print_results(m)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
