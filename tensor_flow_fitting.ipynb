{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# To default to float division and print function.\n",
    "from __future__ import (division, print_function)\n",
    "\n",
    "# Core python libraries\n",
    "import csv\n",
    "import re\n",
    "import math\n",
    "\n",
    "# External libraries.\n",
    "import nltk\n",
    "import numpy\n",
    "import pandas\n",
    "import scipy\n",
    "import sklearn\n",
    "\n",
    "# Tensorflow and related.\n",
    "import tensorflow\n",
    "\n",
    "# For fitting streamlined tensor flow models.\n",
    "import tflearn\n",
    "\n",
    "# So you know when this code block finishes.\n",
    "print (\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n"
     ]
    }
   ],
   "source": [
    "#TODO (max): make a library that does all this preprocessing\n",
    "data = pandas.read_csv('data/train_large.csv')\n",
    "\n",
    "# This is for use in exploring logistic regression. Need a binomial outcome variable\n",
    "LABEL_COLUMN = \"HighPayingJobs\"\n",
    "data[LABEL_COLUMN] = (data[\"SalaryNormalized\"].apply(lambda x: x >= 30000)).astype(int)\n",
    "\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "X_train_index, X_test_index, Y_train, Y_test = sklearn.cross_validation.train_test_split(\n",
    "    data.index, data['LogSalaryNormalized'], test_size=.33, random_state=42)\n",
    "\n",
    "# Keep train and test as pandas dataframes.\n",
    "X_train = data.iloc[X_train_index]\n",
    "X_test = data.iloc[X_test_index]\n",
    "\n",
    "yb_train = X_train[LABEL_COLUMN]\n",
    "yb_test= X_test[LABEL_COLUMN]\n",
    "\n",
    "# Uncomment this next line if you want to check the data.\n",
    "#print(data.head())\n",
    "\n",
    "print (\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13400, 14671)\n",
      "(6600, 14671)\n"
     ]
    }
   ],
   "source": [
    "MIN_WORD_FREQUENCY = 3\n",
    "\n",
    "count_vect = sklearn.feature_extraction.text.CountVectorizer(\n",
    "  stop_words='english', min_df=MIN_WORD_FREQUENCY)\n",
    "\n",
    "# Here I am count vectoring the full description field. But in theory\n",
    "# any text field can be processed this way.\n",
    "X_train_full_description_counts = count_vect.fit_transform(X_train['FullDescriptionWithTitle'])\n",
    "X_test_full_description_counts = count_vect.transform(X_test['FullDescriptionWithTitle'])\n",
    "\n",
    "print(X_train_full_description_counts.shape)\n",
    "print(X_test_full_description_counts.shape)\n",
    "#TODO (max): supress the VisibleDeprecationWarning here from numpy. It is being thrown by the package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean salary value in training set is  £  28152.20\n",
      "Guess the average Mean Absolute Error: 12583.2907\n",
      "SGDRegressor Mean Absolute Error: 13266.4945\n"
     ]
    }
   ],
   "source": [
    "def mean_absolute_error_salary_scale(y_test, y_predicted):\n",
    "    return sklearn.metrics.mean_absolute_error(\n",
    "        numpy.exp(y_test), numpy.exp(y_predicted))\n",
    "\n",
    "# Guess the average. Create an empty vector of the desired shape.\n",
    "average_guess = numpy.empty(Y_test.shape)\n",
    "average_guess.fill(numpy.mean(Y_train))\n",
    "\n",
    "mean_guess = average_guess[0]\n",
    "print('Mean salary value in training set is  £{:10.2f}'.format(math.exp(mean_guess)))\n",
    "average_guess_mae = mean_absolute_error_salary_scale(Y_test, average_guess)\n",
    "print('Guess the average Mean Absolute Error: {:10.4f}'.format(average_guess_mae))\n",
    "\n",
    "# SGD Needs normalized inputs\n",
    "normalizer = sklearn.preprocessing.Normalizer(norm='l1')\n",
    "X_train_norm = normalizer.fit_transform(X_train_full_description_counts.astype('float64'))\n",
    "X_test_norm = normalizer.transform(X_test_full_description_counts.astype('float64'))\n",
    "\n",
    "# We want a stochastic gradient descent with l1 norm.\n",
    "sgd = sklearn.linear_model.SGDRegressor(alpha=.005, penalty='l1', n_iter=100)\n",
    "sgd.fit(X_train_norm, Y_train)\n",
    "sgd_predictions = sgd.predict(X_test_norm)\n",
    "sgd_mae = mean_absolute_error_salary_scale(Y_test, sgd_predictions)\n",
    "print('SGDRegressor Mean Absolute Error: {:10.4f}'.format(sgd_mae))\n",
    "#TODO (any): wonder why this is so inaccurate/wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    7373\n",
      "1    6027\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Will try and do a logistic regression to predict High paying jobs as per\n",
    "# https://www.tensorflow.org/versions/r0.10/tutorials/wide/index.html\n",
    "print(X_train[LABEL_COLUMN].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# feature_cols: A dict from feature column names to Tensors or SparseTensors.\n",
    "# label: A Tensor containing the label column\n",
    "\n",
    "# The keys of the feature_cols will be used to when construct columns in the next\n",
    "# section. Because we want to call the fit and evaluate methods with different \n",
    "# data, we define two different input builder functions, train_input_fn and\n",
    "# test_input_fn which are identical except that they pass different data to\n",
    "# input_fn. Note that input_fn will be called while constructing the TensorFlow\n",
    "# graph, not while running the graph. What it is returning is a representation\n",
    "# of the input data as the fundamental unit of TensorFlow computations, a Tensor\n",
    "# (or SparseTensor).\n",
    "\n",
    "CONTINUOUS_COLUMNS = [\"SalaryNormalized\", \"DescriptionLength\"]\n",
    "\n",
    "CATEGORICAL_COLUMNS = [\"ContractTime\", \"ContractType\"]\n",
    "\n",
    "def input_fn(df):\n",
    "  # Creates a dictionary mapping from each continuous feature column name (k) to\n",
    "  # the values of that column stored in a constant Tensor.\n",
    "  continuous_cols = {k: tensorflow.constant(df[k].values)\n",
    "                     for k in CONTINUOUS_COLUMNS}\n",
    "  # Creates a dictionary mapping from each categorical feature column name (k)\n",
    "  # to the values of that column stored in a tensorflow.SparseTensor.\n",
    "  categorical_cols = {k: tensorflow.SparseTensor(\n",
    "      indices=[[i, 0] for i in range(df[k].size)],\n",
    "      values=df[k].values,\n",
    "      shape=[df[k].size, 1])\n",
    "                      for k in CATEGORICAL_COLUMNS}\n",
    "  # Merges the two dictionaries into one.\n",
    "  feature_cols = dict(continuous_cols.items() + categorical_cols.items())\n",
    "  # Converts the label column into a constant Tensor.\n",
    "  label = tensorflow.constant(df[LABEL_COLUMN].values)\n",
    "  # Returns the feature columns and the label.\n",
    "  return feature_cols, label\n",
    "\n",
    "def train_input_fn():\n",
    "  return input_fn(X_train)\n",
    "\n",
    "def eval_input_fn():\n",
    "  return input_fn(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tempfile\n",
    "\n",
    "# This is a test. It should give 100% accuracy. Something is amiss.\n",
    "salary_normalized = tensorflow.contrib.layers.real_valued_column(\n",
    "    \"SalaryNormalized\")\n",
    "# Real features.\n",
    "description_length = tensorflow.contrib.layers.real_valued_column(\n",
    "    \"DescriptionLength\")\n",
    "contract_time = tensorflow.contrib.layers.sparse_column_with_hash_bucket(\n",
    "    \"ContractTime\", hash_bucket_size=100)\n",
    "contract_type = tensorflow.contrib.layers.sparse_column_with_hash_bucket(\n",
    "    \"ContractType\", hash_bucket_size=100)\n",
    "\n",
    "\n",
    "\n",
    "model_dir = tempfile.mkdtemp()\n",
    "m = tensorflow.contrib.learn.LinearClassifier(feature_columns=[\n",
    "        salary_normalized,\n",
    "        #description_length, contract_time, contract_type\n",
    "    ],\n",
    "    #\"\"\"optimizer=tensorflow.train.FtrlOptimizer(\n",
    "    #learning_rate=0.1,\n",
    "    #l1_regularization_strength=1.0,\n",
    "    #l2_regularization_strength=1.0)\"\"\"\n",
    "  model_dir=model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearClassifier()"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.fit(input_fn=train_input_fn, steps=2000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.546364\n",
      "eval_auc: 0.5\n",
      "loss: 282.223\n"
     ]
    }
   ],
   "source": [
    "results = m.evaluate(input_fn=eval_input_fn, steps=100)\n",
    "for key in sorted(results):\n",
    "    print (\"%s: %s\" % (key, results[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ..., 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "x, y = eval_input_fn()\n",
    "\n",
    "# There is the old issue :D All 0 values :(\n",
    "print(m.predict(input_fn=eval_input_fn))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 25\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "\n",
    "# tf Graph Input\n",
    "x = tf.placeholder(tf.float32, [None, 1]) # mnist data image of shape 28*28=784\n",
    "y = tf.placeholder(tf.float32, [None, 2]) # 0-9 digits recognition => 10 classes\n",
    "\n",
    "# Set model weights\n",
    "W = tf.Variable(tf.zeros([1, 2]))\n",
    "b = tf.Variable(tf.zeros([2]))\n",
    "\n",
    "# Construct model\n",
    "pred = tf.nn.softmax(tf.matmul(x, W) + b) # Softmax\n",
    "\n",
    "# Minimize error using cross entropy\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=1))\n",
    "# Gradient Descent\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "670\n",
      "703    25036\n",
      "311    36480\n",
      "722    39000\n",
      "629    23520\n",
      "0      25000\n",
      "316    18240\n",
      "706    22932\n",
      "547    30263\n",
      "872    23040\n",
      "532    16041\n",
      "477    35500\n",
      "404    45120\n",
      "172    16320\n",
      "125    22617\n",
      "394    17280\n",
      "...\n",
      "87     50000\n",
      "458    17280\n",
      "330    17875\n",
      "214    16320\n",
      "466    28500\n",
      "121    10905\n",
      "614    28000\n",
      "20     25000\n",
      "700    25250\n",
      "71     22000\n",
      "106    50000\n",
      "270    15523\n",
      "860    41280\n",
      "435    17990\n",
      "102    40000\n",
      "Name: SalaryNormalized, Length: 670, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "m = len(X_train)\n",
    "x = X_train['SalaryNormalized']\n",
    "y = \n",
    "\n",
    "\n",
    "print(m)\n",
    "print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-87-94f947ac16cb>, line 20)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-87-94f947ac16cb>\"\u001b[1;36m, line \u001b[1;32m20\u001b[0m\n\u001b[1;33m    print \"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost)\u001b[0m\n\u001b[1;37m                 ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(m/batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            # Fit training using batch data\n",
    "            _, c = sess.run([optimizer, cost], feed_dict={x: batch_xs,\n",
    "                                                          y: batch_ys})\n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "        # Display logs per epoch step\n",
    "        if (epoch+1) % display_step == 0:\n",
    "            print \"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost)\n",
    "\n",
    "    print \"Optimization Finished!\"\n",
    "\n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    # Calculate accuracy for 3000 examples\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    print \"Accuracy:\", accuracy.eval({x: mnist.test.images[:3000], y: mnist.test.labels[:3000]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.array(X_train['SalaryNormalized']).reshape([len(X_train),1])\n",
    "y = np.array(X_train[LABEL_COLUMN]).reshape([len(X_train),1])\n",
    "\n",
    "x_test = np.array(X_test['SalaryNormalized']).reshape([len(X_test),1])\n",
    "y_test = np.array(X_test[LABEL_COLUMN]).reshape([len(X_test),1])\n",
    "\n",
    "# DNN needs normalized inputs\n",
    "normalizer = sklearn.preprocessing.Normalizer(norm='l1')\n",
    "x_norm = normalizer.fit_transform(x.astype('float'))\n",
    "x_test_norm = normalizer.transform(x_test.astype('float'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(670, 1)\n",
      "(670, 1)\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)\n",
    "print(x_norm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.669697\n"
     ]
    }
   ],
   "source": [
    "# Build 3 layer DNN with 10, 20, 10 units respectively.\n",
    "classifier = tf.contrib.learn.DNNClassifier(hidden_units=[10], n_classes=2)\n",
    "\n",
    "# Fit model.\n",
    "classifier.fit(x=x_norm, y=y, steps=2000)\n",
    "\n",
    "# Evaluate accuracy.\n",
    "accuracy_score = classifier.evaluate(x=x_test_norm, y=y_test)[\"accuracy\"]\n",
    "print('Accuracy: {0:f}'.format(accuracy_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
